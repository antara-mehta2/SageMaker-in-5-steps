{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create and start a SageMaker Instance\n",
    "An Amazon SageMaker notebook instance is an ML compute instance running the Jupyter Notebook App. An instance is nothing but a virtual machine where we can choose properties like processors, GPU, RAM, and others, based on the project requirements. To create a notebook instance, use either the SageMaker console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inititate a SageMaker Session\n",
    "SageMaker session is an object that represents the SageMaker session that we are currently operating within. It manages interactions with the Amazon SageMaker APIs and any other AWS services needed. This class provides convenient methods for manipulating entities and resources that Amazon SageMaker uses, such as training jobs, endpoints, and input datasets in S3. We will discuss them in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get execution role\n",
    "Get the notebook instance's execution role, which is the IAM role that we created for our SageMaker notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set s3 bucket and folders\n",
    "Then extract the default bucket assigned to this session, using session method(sagemaker.Session().default_bucket()) or provide an existing bucket name. Let us also create a folder name in the s3 bucket to store all the data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker.Session().default_bucket()\n",
    "s3_prefix = 'spam-data' #prefix used for data stored within the bucket\n",
    "s3_path = 's3://{}/{}/'.format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the data\n",
    "We will begin by uploading data to SageMaker; this can be done in two ways, upload it to a local directory or s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to s3\n",
    "To upload the data to s3, create an s3 bucket. Follow this blog to create a bucket, then upload the data to the bucket, note down the bucket, and file name of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('SMSSpamCollection.txt', sep=\"\\t\", header=None, names = ['labels', 'messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                           messages\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess & split dataset into training and testing\n",
    "Next, convert the textual data into numerical values. We will use sklearn to preprocess the data. \n",
    "- Import all the requisite packages from sklearn library. \n",
    "- Build a Tfidf pipeline to preprocess data. \n",
    "- Convert the label column to numerical values (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = Pipeline([('cv',CountVectorizer()), ('tfidf_transformer',TfidfTransformer(smooth_idf=True,use_idf=True))])\n",
    "tf_idf_vector  = pd.DataFrame(tf_idf.fit_transform(data['messages']).todense())\n",
    "data['labels'] = label_binarize(data['labels'], classes=['ham', 'spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_vector, data['labels'], test_size=0.3, random_state=2020)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload train and test data to S3\n",
    "To upload the data to s3, create a folder in the local directory and save the data in the folder. \n",
    "- Create folder: To create a folder or to ensure that a folder exists, run the following command.\n",
    "    \n",
    "- Save to local directory: Ensure that the header and index are false since that is the format required by the AWS training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "import scipy.sparse\n",
    "# scipy.sparse.save_npz(os.path.join(data_dir, 'test_data.csv'), X_test)\n",
    "X_test.to_csv(os.path.join(data_dir, 'test_data.csv'), header=False, index=False)\n",
    "pd.concat([y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train_data.csv'), header=False, index=False)\n",
    "pd.concat([y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'val_data.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = session.upload_data(os.path.join(data_dir, 'test_data.csv'), key_prefix = s3_prefix)\n",
    "train_path = session.upload_data(os.path.join(data_dir, 'train_data.csv'), key_prefix = s3_prefix)\n",
    "val_path = session.upload_data(os.path.join(data_dir, 'val_data.csv'), key_prefix = s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_test = y_train = X_val = y_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the XGBoost algorithm image\n",
    "\n",
    "Estimators are a high-level interface for SageMaker training to handle end-to-end Amazon SageMaker training and deployment tasks.\n",
    "Estimator object requires three main objects:\n",
    "1. sagemaker_session: We will use the session object that we created in the first section.\n",
    "2. role: We will use the execution role object that we created in the first section.\n",
    "3. model_uri: Next, configure the container image for the region that we are running in. In local mode, this should point to the path in which the model is located and not the file itself, as local Docker containers will try to mount the URI as a volume. model_uri requires two inputs the name of the estimator model, in our case XGBoost, and the region name that can be extracted using the session method (session.boto_region_name) or using boto3 (boto3.Session().region_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "# container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the estimator object\n",
    "To construct an estimator object, we will need to provide an s3 path for the estimator object to save the model. Let us append the name of the model output folder (model_output) to the s3 path (s3_path) that we created in the first section to build a path to save model outputs (artifacts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-2-629866591278/spam-data/model_output'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = s3_path + 'model_output'\n",
    "output_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparamaters\n",
    "We will build a dictionary of the parameters that we would like to define, then feed this dictionary into the estimator object using the set_hyperparameter method in the estimator object. For a detailed understanding of parameters in XGBoost models, refer to this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters ={\n",
    "\"max_depth\": 5,\n",
    "\"eta\": 0.2,\n",
    "\"gamma\": 2,\n",
    "\"min_child_weight\": 5,\n",
    "\"subsample\": 0.8,\n",
    "\"objective\": \"binary:logistic\",\n",
    "\"early_stopping_rounds\": 25,\n",
    "\"num_round\": 150,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the objects and output path required to create an estimator object. For a detailed understanding of other parameters in an estimator object, please refer to this link. We will only be using a few high-level parameters in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "## get estimator\n",
    "classifier = sagemaker.estimator.Estimator(\n",
    "            container,\n",
    "            role,\n",
    "            train_instance_count=1,\n",
    "            train_instance_type='ml.m4.xlarge',\n",
    "            output_path=output_path,\n",
    "            sagemaker_session=session)\n",
    "## set hyperparameters\n",
    "classifier.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit the model\n",
    "With the estimator object set up, SageMaker can now fit the model. We will need to specify the location of the data, where we will provide the URI that points to that data in S3 in the sagemaker.s3_input object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-16 21:02:40 Starting - Starting the training job...\n",
      "2020-09-16 21:02:46 Starting - Launching requested ML instances......\n",
      "2020-09-16 21:03:52 Starting - Preparing the instances for training......\n",
      "2020-09-16 21:04:43 Downloading - Downloading input data...\n",
      "2020-09-16 21:05:35 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:05:35:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:05:35:INFO] File size need to be processed in the node: 130.41mb. Available memory size in the node: 8489.63mb\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:05:35:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[21:05:35] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[21:05:36] 2730x8713 matrix with 23786490 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:05:36:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[21:05:36] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[21:05:36] 1170x8713 matrix with 10194210 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[21:05:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.060806#011validation-error:0.063248\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 25 rounds.\u001b[0m\n",
      "\u001b[34m[21:05:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.060806#011validation-error:0.063248\u001b[0m\n",
      "\u001b[34m[21:05:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.050549#011validation-error:0.049573\u001b[0m\n",
      "\u001b[34m[21:05:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.053114#011validation-error:0.052991\u001b[0m\n",
      "\u001b[34m[21:05:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.049084#011validation-error:0.051282\u001b[0m\n",
      "\u001b[34m[21:05:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.049084#011validation-error:0.049573\u001b[0m\n",
      "\u001b[34m[21:05:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.04652#011validation-error:0.049573\u001b[0m\n",
      "\u001b[34m[21:05:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.045055#011validation-error:0.047009\u001b[0m\n",
      "\u001b[34m[21:05:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.043956#011validation-error:0.045299\u001b[0m\n",
      "\u001b[34m[21:05:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.046154#011validation-error:0.044444\u001b[0m\n",
      "\u001b[34m[21:05:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.043223#011validation-error:0.044444\u001b[0m\n",
      "\u001b[34m[21:05:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.03956#011validation-error:0.040171\u001b[0m\n",
      "\u001b[34m[21:05:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.037363#011validation-error:0.040171\u001b[0m\n",
      "\u001b[34m[21:05:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.035531#011validation-error:0.035897\u001b[0m\n",
      "\u001b[34m[21:05:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.035531#011validation-error:0.035897\u001b[0m\n",
      "\u001b[34m[21:05:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.033333#011validation-error:0.034188\u001b[0m\n",
      "\u001b[34m[21:05:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.032967#011validation-error:0.033333\u001b[0m\n",
      "\u001b[34m[21:05:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.0337#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.034066#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.0337#011validation-error:0.029915\u001b[0m\n",
      "\u001b[34m[21:05:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.031502#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.030403#011validation-error:0.029915\u001b[0m\n",
      "\u001b[34m[21:05:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.030403#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.030037#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.027839#011validation-error:0.028205\u001b[0m\n",
      "\u001b[34m[21:05:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.027473#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.028205#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.027106#011validation-error:0.029915\u001b[0m\n",
      "\u001b[34m[21:05:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.027106#011validation-error:0.031624\u001b[0m\n",
      "\u001b[34m[21:05:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.027106#011validation-error:0.02735\u001b[0m\n",
      "\u001b[34m[21:05:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.026374#011validation-error:0.02735\u001b[0m\n",
      "\u001b[34m[21:05:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.024176#011validation-error:0.026496\u001b[0m\n",
      "\u001b[34m[21:05:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.02381#011validation-error:0.025641\u001b[0m\n",
      "\u001b[34m[21:05:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.02381#011validation-error:0.025641\u001b[0m\n",
      "\u001b[34m[21:05:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.023443#011validation-error:0.025641\u001b[0m\n",
      "\u001b[34m[21:05:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.021978#011validation-error:0.026496\u001b[0m\n",
      "\u001b[34m[21:05:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.022344#011validation-error:0.028205\u001b[0m\n",
      "\u001b[34m[21:05:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.022344#011validation-error:0.028205\u001b[0m\n",
      "\u001b[34m[21:05:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.021612#011validation-error:0.02735\u001b[0m\n",
      "\u001b[34m[21:05:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.021612#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.021612#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.021612#011validation-error:0.028205\u001b[0m\n",
      "\u001b[34m[21:05:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 2 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.021978#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 10 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.022344#011validation-error:0.028205\u001b[0m\n",
      "\u001b[34m[21:05:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 10 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.021978#011validation-error:0.029915\u001b[0m\n",
      "\u001b[34m[21:05:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.021612#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.021245#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.021978#011validation-error:0.029915\u001b[0m\n",
      "\n",
      "2020-09-16 21:06:08 Uploading - Uploading generated training model\n",
      "2020-09-16 21:06:08 Completed - Training job completed\n",
      "\u001b[34m[21:05:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.021978#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.021978#011validation-error:0.02906\u001b[0m\n",
      "\u001b[34m[21:05:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.021612#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.021612#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 8 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.021612#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 6 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.021245#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.021245#011validation-error:0.030769\u001b[0m\n",
      "\u001b[34m[21:05:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 6 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.020879#011validation-error:0.028205\u001b[0m\n",
      "\u001b[34m[21:05:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 6 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.020513#011validation-error:0.029915\u001b[0m\n",
      "\u001b[34m[21:05:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 4 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.01978#011validation-error:0.029915\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.02381#011validation-error:0.025641\n",
      "\u001b[0m\n",
      "Training seconds: 85\n",
      "Billable seconds: 85\n"
     ]
    }
   ],
   "source": [
    "s3_train = sagemaker.s3_input(s3_data=train_path, content_type='csv')\n",
    "s3_val = sagemaker.s3_input(s3_data=val_path, content_type='csv')\n",
    "classifier.fit({\n",
    "               'train':s3_train,\n",
    "               'validation':s3_val,\n",
    "               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Model\n",
    "### Batch Transform\n",
    "To test the model that we created, we will use SageMaker's Batch Transform functionality, which will split the test data into batches, send it to the model, and merge the results. \n",
    "\n",
    "### Transformer object\n",
    "To start with, build a transformer object to fit the model that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "classifier_transformer = classifier.transformer(instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transform job\n",
    "SageMaker will begin a batch transform job using our trained model and apply it to the test data stored in s3. We will need to provide pieces of information like data location, data type (to serialize data), and split type (to split data into batches). SageMaker will run the batch transform job in the background. To get some output on the job performance, use the wait method in the transformer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_transformer.transform(test_path, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................\u001b[32m2020-09-16T21:10:58.600:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[35mArguments: serve\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-09-16 21:10:58 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:10:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[35m[2020-09-16 21:10:58 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:10:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-09-16:21:11:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-09-16:21:11:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-west-2-629866591278/xgboost-2020-09-16-21-06-22-378/test_data.csv.out to data/test_data.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $classifier_transformer.output_path $data_dir\n",
    "predictions = pd.read_csv(os.path.join(data_dir, 'test_data.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9712918660287081"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
